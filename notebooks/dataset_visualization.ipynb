{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GLiNER Dataset Integration Visualization\n",
        "\n",
        "This notebook breaks down the dataset processing logic for GLiNER integration step-by-step.\n",
        "We use a specific example provided to visualize how `tools`, `messages` are converted into `input_ids`, `span_idx`, and `labels`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6848adef",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Working Directory: /home/namdv/workspace/TinyRecursiveModels\n",
            "Tokenizer Path: data/processed/tokenizer.json\n",
            "Config and Tokenizer loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "# Fix paths: Notebook is in 'notebooks/', strictly move to project root\n",
        "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
        "    os.chdir(\"..\")\n",
        "    \n",
        "# Add project root to path (current dir after chdir)\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "from src.config import Config\n",
        "config = Config()\n",
        "\n",
        "# Verify paths\n",
        "print(f\"Current Working Directory: {os.getcwd()}\")\n",
        "print(f\"Tokenizer Path: {config.tokenizer_path}\")\n",
        "\n",
        "try:\n",
        "    tokenizer = Tokenizer.from_file(config.tokenizer_path)\n",
        "    print(\"Config and Tokenizer loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")\n",
        "    # Fallback/Debug info\n",
        "    print(f\"Files in data/processed: {os.listdir('data/processed') if os.path.exists('data/processed') else 'Not Found'}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "77c97ab2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "512\n"
          ]
        }
      ],
      "source": [
        "print(config.model.max_seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cea0bbce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded sample item.\n"
          ]
        }
      ],
      "source": [
        "# The sample item provided by the user\n",
        "item = {\n",
        "  \"tools\": \"[{\\\"type\\\": \\\"function\\\", \\\"function\\\": {\\\"name\\\": \\\"peers\\\", \\\"description\\\": \\\"Retrieves a list of company peers given a stock symbol.\\\", \\\"parameters\\\": {\\\"symbol\\\": {\\\"description\\\": \\\"The stock symbol for the company.\\\", \\\"type\\\": \\\"str\\\", \\\"default\\\": \\\"\\\"}}}}, {\\\"type\\\": \\\"function\\\", \\\"function\\\": {\\\"name\\\": \\\"web_chain_details\\\", \\\"description\\\": \\\"python\\\", \\\"parameters\\\": {\\\"chain_slug\\\": {\\\"description\\\": \\\"The slug identifier for the blockchain (e.g., 'ethereum' for Ethereum mainnet).\\\", \\\"type\\\": \\\"str\\\", \\\"default\\\": \\\"ethereum\\\"}}}}]\",\n",
        "  \"messages\": [\n",
        "    {\"role\": \"user\", \"content\": \"I need to understand the details of the Ethereum blockchain for my cryptocurrency project. Can you fetch the details for 'ethereum'?\"},\n",
        "    {\"role\": \"tool_call\", \"content\": \"{\\\"name\\\": \\\"web_chain_details\\\", \\\"arguments\\\": {\\\"chain_slug\\\": \\\"ethereum\\\"}}\"}\n",
        "  ]\n",
        "}\n",
        "\n",
        "tools_str = item['tools']\n",
        "messages = item['messages']\n",
        "\n",
        "print(\"Loaded sample item.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "187e9f9e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "STEP 1: PARSING PROMPTS\n",
            "----------------------------------------\n",
            "Extracted Label Names (Classes for GLiNER): ['symbol', 'chain_slug']\n",
            "\n",
            "Encoded Prompts (Token IDs):\n",
            "  'symbol' -> [197]\n",
            "  'chain_slug' -> [3323, 78, 7068]\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Parsing Prompts (Classes)\n",
        "print(\"-\" * 40 + \"\\nSTEP 1: PARSING PROMPTS\\n\" + \"-\" * 40)\n",
        "\n",
        "try:\n",
        "    tools_list = json.loads(tools_str)\n",
        "    label_names = []\n",
        "    for t in tools_list:\n",
        "        if \"function\" in t:\n",
        "            func = t[\"function\"]\n",
        "            if \"parameters\" in func and \"properties\" in func[\"parameters\"]: \n",
        "                    label_names.extend(func[\"parameters\"][\"properties\"].keys())\n",
        "            elif \"parameters\" in func: \n",
        "                    label_names.extend(func[\"parameters\"].keys())\n",
        "    label_names = list(dict.fromkeys(label_names))\n",
        "except:\n",
        "    label_names = [\"unknown\"]\n",
        "\n",
        "print(f\"Extracted Label Names (Classes for GLiNER): {label_names}\")\n",
        "\n",
        "prompts_ids_list = [tokenizer.encode(name).ids for name in label_names]\n",
        "print(\"\\nEncoded Prompts (Token IDs):\")\n",
        "for name, p_ids in zip(label_names, prompts_ids_list):\n",
        "    print(f\"  '{name}' -> {p_ids}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "dd5a0e6a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "STEP 2: SYSTEM PROMPT\n",
            "----------------------------------------\n",
            "System IDs Length: 205\n"
          ]
        }
      ],
      "source": [
        "# Step 2: System Prompt & Init\n",
        "print(\"-\" * 40 + \"\\nSTEP 2: SYSTEM PROMPT\\n\" + \"-\" * 40)\n",
        "\n",
        "all_input_ids = []\n",
        "all_labels = []\n",
        "\n",
        "system_text = \"<|im_start|>system \"\n",
        "system_text += \"You are a helpful assistant. \"\n",
        "system_text += \"# Tools \"\n",
        "system_text += \"You may call one or more functions to assist with the user query. \"\n",
        "system_text += \"You are provided with function signatures within <tools></tools> XML tags: \"\n",
        "system_text += f\"<tools> {tools_str} </tools> \"\n",
        "system_text += \"For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags: \"\n",
        "system_text += \"<tool_call> {\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>} </tool_call><|im_end|> \"\n",
        "\n",
        "system_ids = tokenizer.encode(system_text).ids\n",
        "all_input_ids.extend(system_ids)\n",
        "all_labels.extend([-100] * len(system_ids))\n",
        "\n",
        "print(f\"System IDs Length: {len(system_ids)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e302f089",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "STEP 3: MESSAGES & SPANS\n",
            "----------------------------------------\n",
            "\n",
            "Processing Role: user\n",
            "\n",
            "Processing Role: tool_call\n",
            "  > Full Content Text: <tool_call> {\"name\": \"web_chain_details\", \"arguments\": {\"chain_slug\": \"ethereum\"}} </tool_call><|im_end|> \n",
            "  > Content IDs Length: 19\n",
            "    FOUND SPAN: 'ethereum' at Chars [71:78] for Label 'chain_slug'\n",
            "    -> MAPPED TO TOKENS (Local): [14:14]\n",
            "    -> ABSOLUTE SPAN INDICES: [249, 249]\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Process Messages and Find Spans\n",
        "print(\"-\" * 40 + \"\\nSTEP 3: MESSAGES & SPANS\\n\" + \"-\" * 40)\n",
        "\n",
        "span_indices = []\n",
        "span_label_ids = []\n",
        "\n",
        "for msg in messages:\n",
        "    role = msg['role']\n",
        "    content = msg['content']\n",
        "    print(f\"\\nProcessing Role: {role}\")\n",
        "    \n",
        "    if role == 'tool_call':\n",
        "        # Header <|im_start|>assistant\n",
        "        header_text = \"<|im_start|>assistant \"\n",
        "        header_ids = tokenizer.encode(header_text).ids\n",
        "        all_input_ids.extend(header_ids)\n",
        "        all_labels.extend([-100] * len(header_ids))\n",
        "        \n",
        "        # Content <tool_call> ... </tool_call>\n",
        "        prefix_text = \"<tool_call> \"\n",
        "        suffix_text = \" </tool_call><|im_end|> \"\n",
        "        full_content_text = prefix_text + content + suffix_text\n",
        "        \n",
        "        content_encoding = tokenizer.encode(full_content_text)\n",
        "        content_ids = content_encoding.ids\n",
        "        \n",
        "        base_offset = len(all_input_ids)\n",
        "        all_input_ids.extend(content_ids)\n",
        "        all_labels.extend(content_ids) # Train on this content\n",
        "        \n",
        "        print(f\"  > Full Content Text: {full_content_text}\")\n",
        "        print(f\"  > Content IDs Length: {len(content_ids)}\")\n",
        "        \n",
        "        # FIND SPANS\n",
        "        tool_call_json = json.loads(content)\n",
        "        if \"arguments\" in tool_call_json:\n",
        "            args = tool_call_json[\"arguments\"]\n",
        "            for arg_name, arg_value in args.items():\n",
        "                if arg_name in label_names:\n",
        "                    label_idx = label_names.index(arg_name)\n",
        "                    val_str = str(arg_value)\n",
        "                    \n",
        "                    # Search\n",
        "                    search_start = len(prefix_text)\n",
        "                    char_start = full_content_text.find(val_str, search_start)\n",
        "                    \n",
        "                    if char_start != -1:\n",
        "                        char_end = char_start + len(val_str) - 1\n",
        "                        print(f\"    FOUND SPAN: '{val_str}' at Chars [{char_start}:{char_end}] for Label '{arg_name}'\")\n",
        "                        \n",
        "                        token_start = content_encoding.char_to_token(char_start)\n",
        "                        token_end = content_encoding.char_to_token(char_end)\n",
        "                        \n",
        "                        print(f\"    -> MAPPED TO TOKENS (Local): [{token_start}:{token_end}]\")\n",
        "                        \n",
        "                        if token_start is not None and token_end is not None:\n",
        "                            abs_start = base_offset + token_start\n",
        "                            abs_end = base_offset + token_end\n",
        "                            span_indices.append([abs_start, abs_end])\n",
        "                            span_label_ids.append(label_idx)\n",
        "                            print(f\"    -> ABSOLUTE SPAN INDICES: [{abs_start}, {abs_end}]\")\n",
        "    else:\n",
        "        # User/System\n",
        "        text = f\"<|im_start|>{role} {content}<|im_end|> \"\n",
        "        ids = tokenizer.encode(text).ids\n",
        "        all_input_ids.extend(ids)\n",
        "        all_labels.extend([-100] * len(ids))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8d2d792a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "STEP 4: BOS/EOS & SHIFT\n",
            "----------------------------------------\n",
            "Span Indices Shifted (+1 for BOS):\n",
            "  [249, 249] -> [250, 250]\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Add BOS/EOS and Shift Spans\n",
        "print(\"-\" * 40 + \"\\nSTEP 4: BOS/EOS & SHIFT\\n\" + \"-\" * 40)\n",
        "\n",
        "bos_token_id = tokenizer.token_to_id(\"<s>\")\n",
        "eos_token_id = tokenizer.token_to_id(\"</s>\")\n",
        "\n",
        "# Prepend BOS\n",
        "all_input_ids = [bos_token_id] + all_input_ids\n",
        "all_labels = [-100] + all_labels\n",
        "\n",
        "# SHIFT SPANS BY +1\n",
        "old_spans = list(span_indices)\n",
        "span_indices = [[s[0]+1, s[1]+1] for s in span_indices]\n",
        "\n",
        "print(f\"Span Indices Shifted (+1 for BOS):\")\n",
        "for old, new in zip(old_spans, span_indices):\n",
        "    print(f\"  {old} -> {new}\")\n",
        "\n",
        "# Append EOS\n",
        "all_input_ids = all_input_ids + [eos_token_id]\n",
        "all_labels = all_labels + [eos_token_id]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7ac26749",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "STEP 5: FINAL OUTPUT\n",
            "----------------------------------------\n",
            "Using Max Sequence Length from Config: 512\n",
            "Final Input IDs Shape: torch.Size([512])\n",
            "Final Labels Shape: torch.Size([512])\n",
            "Final Spans: [[250, 250]]\n",
            "Final Span Labels: [1]\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Final Tensors (Padding)\n",
        "print(\"-\" * 40 + \"\\nSTEP 5: FINAL OUTPUT\\n\" + \"-\" * 40)\n",
        "\n",
        "MAX_SEQ_LEN = config.model.max_seq_len\n",
        "print(f\"Using Max Sequence Length from Config: {MAX_SEQ_LEN}\")\n",
        "\n",
        "ids = all_input_ids\n",
        "labels = all_labels\n",
        "\n",
        "# Pad\n",
        "padding_len = MAX_SEQ_LEN - len(ids)\n",
        "if padding_len > 0:\n",
        "    ids = ids + [tokenizer.token_to_id(\"<pad>\")] * padding_len\n",
        "    labels = labels + [-100] * padding_len\n",
        "else:\n",
        "    # Truncate if needed\n",
        "    ids = ids[:MAX_SEQ_LEN]\n",
        "    labels = labels[:MAX_SEQ_LEN]\n",
        "\n",
        "input_tensor = torch.tensor(ids)\n",
        "labels_tensor = torch.tensor(labels)\n",
        "\n",
        "print(f\"Final Input IDs Shape: {input_tensor.shape}\")\n",
        "print(f\"Final Labels Shape: {labels_tensor.shape}\")\n",
        "print(f\"Final Spans: {span_indices}\")\n",
        "print(f\"Final Span Labels: {span_label_ids}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
